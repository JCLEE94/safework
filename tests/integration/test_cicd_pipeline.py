"""
CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸
CI/CD Pipeline Integration Tests
"""

import asyncio
import json
import os
import subprocess
from datetime import datetime, timedelta

import pytest
from fastapi.testclient import TestClient

from src.app import create_app


class TestCICDPipeline:
    """CI/CD íŒŒì´í”„ë¼ì¸ ë° ë°°í¬ í”„ë¡œì„¸ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def test_client(self):
        """í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸"""
        app = create_app()
        return TestClient(app)
    
    @pytest.fixture
    def sample_pipeline_config(self):
        """ìƒ˜í”Œ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        return {
            "pipeline_name": "safework-main",
            "triggers": {
                "push": {
                    "branches": ["main", "develop"],
                    "paths": ["src/**", "tests/**", "docker/**"]
                },
                "pull_request": {
                    "branches": ["main"],
                    "types": ["opened", "synchronize", "reopened"]
                },
                "schedule": {
                    "cron": "0 2 * * *",  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
                    "branch": "main"
                }
            },
            "stages": [
                {
                    "name": "test",
                    "jobs": ["unit_tests", "integration_tests", "security_scan"]
                },
                {
                    "name": "build",
                    "jobs": ["docker_build", "frontend_build"]
                },
                {
                    "name": "deploy",
                    "jobs": ["deploy_staging", "deploy_production"],
                    "manual_approval": True
                }
            ],
            "environment_variables": {
                "DOCKER_REGISTRY": "registry.jclee.me",
                "PROJECT_NAME": "safework",
                "SLACK_WEBHOOK": "${secrets.SLACK_WEBHOOK}"
            }
        }
    
    async def test_pipeline_execution_workflow(self, test_client, sample_pipeline_config):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        
        # 1. íŒŒì´í”„ë¼ì¸ íŠ¸ë¦¬ê±° ì´ë²¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜
        trigger_event = {
            "event_type": "push",
            "branch": "main",
            "commit": {
                "sha": "abc123def456",
                "message": "feat: ìƒˆë¡œìš´ ì•ˆì „ ì ê²€ ê¸°ëŠ¥ ì¶”ê°€",
                "author": "developer@company.com",
                "timestamp": datetime.now().isoformat()
            },
            "changed_files": [
                "src/handlers/safety_inspection.py",
                "tests/test_safety_inspection.py"
            ]
        }
        
        response = test_client.post("/api/v1/pipeline/trigger", json=trigger_event)
        assert response.status_code == 202  # Accepted
        
        pipeline_run_id = response.json()["run_id"]
        
        # 2. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìƒíƒœ í™•ì¸
        response = test_client.get(f"/api/v1/pipeline/runs/{pipeline_run_id}")
        assert response.status_code == 200
        
        run_status = response.json()
        assert run_status["status"] in ["queued", "running", "completed", "failed"]
        assert run_status["trigger"]["event_type"] == "push"
        
        # 3. ê°œë³„ ì‘ì—… ìƒíƒœ ëª¨ë‹ˆí„°ë§
        response = test_client.get(f"/api/v1/pipeline/runs/{pipeline_run_id}/jobs")
        assert response.status_code == 200
        
        jobs = response.json()
        expected_jobs = ["unit_tests", "integration_tests", "security_scan", 
                        "docker_build", "frontend_build"]
        
        for job_name in expected_jobs:
            assert any(job["name"] == job_name for job in jobs)
        
        # 4. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¡°íšŒ
        response = test_client.get(f"/api/v1/pipeline/runs/{pipeline_run_id}/test-results")
        assert response.status_code == 200
        
        test_results = response.json()
        assert "unit_tests" in test_results
        assert "integration_tests" in test_results
        assert "coverage" in test_results
        
        # 5. ë³´ì•ˆ ìŠ¤ìº” ê²°ê³¼
        response = test_client.get(f"/api/v1/pipeline/runs/{pipeline_run_id}/security-scan")
        assert response.status_code == 200
        
        security_results = response.json()
        assert "vulnerabilities" in security_results
        assert "severity_summary" in security_results
        
        return pipeline_run_id
    
    async def test_docker_build_and_registry_push(self, test_client):
        """Docker ë¹Œë“œ ë° ë ˆì§€ìŠ¤íŠ¸ë¦¬ í‘¸ì‹œ í…ŒìŠ¤íŠ¸"""
        
        # 1. Docker ë¹Œë“œ ì‘ì—… ì‹œì‘
        build_config = {
            "dockerfile": "Dockerfile",
            "context": ".",
            "build_args": {
                "PYTHON_VERSION": "3.11",
                "NODE_VERSION": "18"
            },
            "tags": [
                "safework:latest",
                "safework:v1.2.0",
                f"safework:build-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
            ],
            "cache_from": ["safework:latest"],
            "platforms": ["linux/amd64", "linux/arm64"]
        }
        
        response = test_client.post("/api/v1/pipeline/docker/build", json=build_config)
        assert response.status_code == 202
        
        build_job_id = response.json()["job_id"]
        
        # 2. ë¹Œë“œ ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°
        response = test_client.get(f"/api/v1/pipeline/docker/build/{build_job_id}/logs")
        assert response.status_code == 200
        
        # 3. ì´ë¯¸ì§€ ìŠ¤ìº”
        scan_config = {
            "image": "safework:latest",
            "scan_types": ["vulnerabilities", "secrets", "licenses"],
            "severity_threshold": "HIGH",
            "fail_on_severity": True
        }
        
        response = test_client.post("/api/v1/pipeline/docker/scan", json=scan_config)
        assert response.status_code == 200
        
        scan_results = response.json()
        assert "vulnerabilities" in scan_results
        assert "secrets" in scan_results
        assert "licenses" in scan_results
        
        # 4. ë ˆì§€ìŠ¤íŠ¸ë¦¬ í‘¸ì‹œ
        push_config = {
            "image": "safework:latest",
            "registry": "registry.jclee.me",
            "repository": "safework",
            "tags": ["latest", "v1.2.0"],
            "sign_image": True
        }
        
        response = test_client.post("/api/v1/pipeline/docker/push", json=push_config)
        assert response.status_code == 202
        
        push_job_id = response.json()["job_id"]
        
        # 5. í‘¸ì‹œ ì™„ë£Œ í™•ì¸
        response = test_client.get(f"/api/v1/pipeline/docker/push/{push_job_id}/status")
        assert response.status_code == 200
        
        push_status = response.json()
        assert push_status["status"] in ["pushing", "completed", "failed"]
        if push_status["status"] == "completed":
            assert "manifest_digest" in push_status
    
    async def test_kubernetes_deployment_with_argocd(self, test_client):
        """Kubernetes ë°°í¬ ë° ArgoCD ë™ê¸°í™” í…ŒìŠ¤íŠ¸"""
        
        # 1. Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        manifest_update = {
            "environment": "production",
            "updates": {
                "deployment": {
                    "image": "registry.jclee.me/safework:v1.2.0",
                    "replicas": 3,
                    "resources": {
                        "requests": {"cpu": "500m", "memory": "1Gi"},
                        "limits": {"cpu": "2000m", "memory": "4Gi"}
                    }
                },
                "configmap": {
                    "DATABASE_URL": "${secrets.PROD_DATABASE_URL}",
                    "REDIS_URL": "${secrets.PROD_REDIS_URL}"
                }
            },
            "git_commit_message": "chore: update production deployment to v1.2.0"
        }
        
        response = test_client.post("/api/v1/pipeline/k8s/update-manifests", json=manifest_update)
        assert response.status_code == 200
        
        commit_sha = response.json()["commit_sha"]
        
        # 2. ArgoCD ë™ê¸°í™” íŠ¸ë¦¬ê±°
        argocd_sync = {
            "application": "safework-production",
            "revision": commit_sha,
            "prune": True,
            "dry_run": False,
            "sync_options": ["CreateNamespace=true", "PruneLast=true"]
        }
        
        response = test_client.post("/api/v1/pipeline/argocd/sync", json=argocd_sync)
        assert response.status_code == 202
        
        sync_operation_id = response.json()["operation_id"]
        
        # 3. ë™ê¸°í™” ìƒíƒœ ëª¨ë‹ˆí„°ë§
        response = test_client.get(f"/api/v1/pipeline/argocd/sync/{sync_operation_id}")
        assert response.status_code == 200
        
        sync_status = response.json()
        assert sync_status["phase"] in ["Running", "Succeeded", "Failed", "Error"]
        assert "resources" in sync_status
        
        # 4. í—¬ìŠ¤ ì²´í¬
        health_check_config = {
            "environment": "production",
            "checks": [
                {
                    "type": "http",
                    "url": "https://safework.jclee.me/health",
                    "expected_status": 200,
                    "timeout": 30
                },
                {
                    "type": "database",
                    "connection_string": "${secrets.PROD_DATABASE_URL}",
                    "query": "SELECT 1"
                },
                {
                    "type": "redis",
                    "connection_string": "${secrets.PROD_REDIS_URL}",
                    "command": "PING"
                }
            ],
            "retry_count": 3,
            "retry_interval": 10
        }
        
        response = test_client.post("/api/v1/pipeline/health-check", json=health_check_config)
        assert response.status_code == 200
        
        health_results = response.json()
        assert all(check["status"] == "healthy" for check in health_results["checks"])
        
        # 5. ë¡¤ë°± ì¤€ë¹„
        rollback_config = {
            "environment": "production",
            "target_revision": "previous",
            "reason": "í…ŒìŠ¤íŠ¸ ë¡¤ë°±",
            "auto_rollback_on_failure": True
        }
        
        response = test_client.post("/api/v1/pipeline/prepare-rollback", json=rollback_config)
        assert response.status_code == 200
        
        rollback_plan = response.json()
        assert "previous_version" in rollback_plan
        assert "rollback_steps" in rollback_plan
    
    async def test_monitoring_and_alerting_integration(self, test_client):
        """ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ í†µí•© í…ŒìŠ¤íŠ¸"""
        
        # 1. íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        response = test_client.get("/api/v1/pipeline/metrics")
        assert response.status_code == 200
        
        metrics = response.json()
        assert "pipeline_duration_avg" in metrics
        assert "success_rate" in metrics
        assert "deployment_frequency" in metrics
        assert "lead_time_for_changes" in metrics
        assert "mttr" in metrics  # Mean Time To Recovery
        
        # 2. ì•Œë¦¼ ê·œì¹™ ì„¤ì •
        alert_rules = {
            "rules": [
                {
                    "name": "pipeline_failure",
                    "condition": "pipeline.status == 'failed'",
                    "severity": "critical",
                    "channels": ["slack", "email"],
                    "message_template": "ğŸš¨ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {{pipeline.name}} ({{pipeline.branch}})"
                },
                {
                    "name": "long_running_pipeline",
                    "condition": "pipeline.duration > 3600",
                    "severity": "warning",
                    "channels": ["slack"],
                    "message_template": "â° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼: {{pipeline.duration_minutes}}ë¶„"
                },
                {
                    "name": "security_vulnerability",
                    "condition": "security_scan.high_severity_count > 0",
                    "severity": "high",
                    "channels": ["slack", "email", "jira"],
                    "message_template": "ğŸ”’ ë³´ì•ˆ ì·¨ì•½ì  ë°œê²¬: {{security_scan.high_severity_count}}ê°œì˜ ê³ ìœ„í—˜ ì·¨ì•½ì "
                }
            ]
        }
        
        response = test_client.post("/api/v1/pipeline/alerts/rules", json=alert_rules)
        assert response.status_code == 201
        
        # 3. ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ
        response = test_client.get("/api/v1/pipeline/dashboard")
        assert response.status_code == 200
        
        dashboard_data = response.json()
        assert "recent_pipelines" in dashboard_data
        assert "deployment_status" in dashboard_data
        assert "quality_metrics" in dashboard_data
        
        # 4. ë¹„ìš© ë¶„ì„
        response = test_client.get("/api/v1/pipeline/cost-analysis?period=monthly")
        assert response.status_code == 200
        
        cost_analysis = response.json()
        assert "compute_cost" in cost_analysis
        assert "storage_cost" in cost_analysis
        assert "network_cost" in cost_analysis
        assert "cost_per_deployment" in cost_analysis
    
    async def test_gitops_workflow(self, test_client):
        """GitOps ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        
        # 1. Git ì €ì¥ì†Œ êµ¬ì¡° í™•ì¸
        response = test_client.get("/api/v1/pipeline/gitops/repository-structure")
        assert response.status_code == 200
        
        repo_structure = response.json()
        expected_dirs = ["k8s/", "helm/", "argocd/", ".github/workflows/"]
        for dir in expected_dirs:
            assert dir in repo_structure["directories"]
        
        # 2. í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬
        env_config = {
            "environment": "staging",
            "configurations": {
                "replicas": 2,
                "database": {
                    "host": "staging-db.internal",
                    "pool_size": 10
                },
                "features": {
                    "experimental_features": True,
                    "debug_mode": True
                }
            },
            "secrets": {
                "database_password": {"from": "vault", "path": "staging/db/password"},
                "jwt_secret": {"from": "sealed-secrets", "name": "jwt-secret-staging"}
            }
        }
        
        response = test_client.post("/api/v1/pipeline/gitops/environment-config", json=env_config)
        assert response.status_code == 200
        
        # 3. Pull Request ìë™í™”
        pr_automation = {
            "source_branch": "feature/new-safety-module",
            "target_branch": "main",
            "title": "feat: ìƒˆë¡œìš´ ì•ˆì „ ëª¨ë“ˆ ë°°í¬",
            "description": """
            ## ë³€ê²½ì‚¬í•­
            - ìƒˆë¡œìš´ ì•ˆì „ ì ê²€ ëª¨ë“ˆ ì¶”ê°€
            - ê´€ë ¨ Kubernetes ë¦¬ì†ŒìŠ¤ ì—…ë°ì´íŠ¸
            
            ## í…ŒìŠ¤íŠ¸
            - [x] ìœ ë‹› í…ŒìŠ¤íŠ¸ í†µê³¼
            - [x] í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼
            - [x] ìŠ¤í…Œì´ì§• í™˜ê²½ ê²€ì¦
            """,
            "labels": ["deployment", "feature"],
            "reviewers": ["devops-team", "safety-team"],
            "auto_merge": {
                "enabled": True,
                "merge_method": "squash",
                "required_approvals": 2,
                "required_checks": ["ci/build", "ci/test", "security-scan"]
            }
        }
        
        response = test_client.post("/api/v1/pipeline/gitops/create-pr", json=pr_automation)
        assert response.status_code == 201
        
        pr_number = response.json()["pull_request_number"]
        
        # 4. ë°°í¬ ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš°
        approval_request = {
            "deployment_id": "deploy-123456",
            "environment": "production",
            "version": "v1.2.0",
            "changes_summary": [
                "ìƒˆë¡œìš´ ì•ˆì „ ì ê²€ ê¸°ëŠ¥",
                "ì„±ëŠ¥ ê°œì„ ",
                "ë²„ê·¸ ìˆ˜ì •"
            ],
            "risk_assessment": "low",
            "rollback_plan": "ì´ì „ ë²„ì „ìœ¼ë¡œ ì¦‰ì‹œ ë¡¤ë°± ê°€ëŠ¥",
            "approvers": ["cto@company.com", "safety-manager@company.com"]
        }
        
        response = test_client.post("/api/v1/pipeline/approval/request", json=approval_request)
        assert response.status_code == 201
        
        approval_id = response.json()["approval_id"]
        
        # 5. ë°°í¬ ì´ë ¥ ì¶”ì 
        response = test_client.get("/api/v1/pipeline/deployments/history?environment=production&limit=10")
        assert response.status_code == 200
        
        deployment_history = response.json()
        assert len(deployment_history) >= 0
        
        for deployment in deployment_history:
            assert "deployment_id" in deployment
            assert "version" in deployment
            assert "deployed_at" in deployment
            assert "deployed_by" in deployment
            assert "status" in deployment
    
    async def test_pipeline_optimization_and_caching(self, test_client):
        """íŒŒì´í”„ë¼ì¸ ìµœì í™” ë° ìºì‹± í…ŒìŠ¤íŠ¸"""
        
        # 1. ë¹Œë“œ ìºì‹œ ì„¤ì •
        cache_config = {
            "cache_layers": [
                {
                    "name": "dependency_cache",
                    "paths": ["node_modules/", "venv/", ".pip/"],
                    "key_pattern": "deps-{{ checksum 'package-lock.json' }}-{{ checksum 'requirements.txt' }}",
                    "restore_keys": ["deps-"]
                },
                {
                    "name": "build_cache",
                    "paths": ["dist/", "build/"],
                    "key_pattern": "build-{{ .Branch }}-{{ .Revision }}",
                    "max_size_mb": 500
                }
            ],
            "docker_cache": {
                "type": "registry",
                "cache_from": ["registry.jclee.me/safework:cache"],
                "cache_to": ["type=registry,ref=registry.jclee.me/safework:cache,mode=max"]
            }
        }
        
        response = test_client.post("/api/v1/pipeline/cache/configure", json=cache_config)
        assert response.status_code == 200
        
        # 2. ë³‘ë ¬ ì‹¤í–‰ ìµœì í™”
        parallel_config = {
            "test_parallelization": {
                "enabled": True,
                "strategy": "auto",
                "max_parallel_jobs": 4,
                "test_splitting": {
                    "method": "timing",
                    "history_days": 30
                }
            },
            "build_matrix": {
                "python_versions": ["3.9", "3.10", "3.11"],
                "node_versions": ["16", "18"],
                "exclude": [
                    {"python_version": "3.9", "node_version": "18"}
                ]
            }
        }
        
        response = test_client.post("/api/v1/pipeline/optimization/parallel", json=parallel_config)
        assert response.status_code == 200
        
        # 3. íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë¶„ì„
        response = test_client.get("/api/v1/pipeline/performance/analysis")
        assert response.status_code == 200
        
        performance_analysis = response.json()
        assert "bottlenecks" in performance_analysis
        assert "optimization_suggestions" in performance_analysis
        assert "time_saved_estimate" in performance_analysis
        
        # 4. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        response = test_client.get("/api/v1/pipeline/resources/usage")
        assert response.status_code == 200
        
        resource_usage = response.json()
        assert "cpu_usage" in resource_usage
        assert "memory_usage" in resource_usage
        assert "disk_usage" in resource_usage
        assert "network_usage" in resource_usage
        
        # 5. ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •
        autoscaling_config = {
            "runners": {
                "min_instances": 1,
                "max_instances": 10,
                "scale_up_threshold": {
                    "pending_jobs": 5,
                    "wait_time_seconds": 60
                },
                "scale_down_threshold": {
                    "idle_time_minutes": 10
                },
                "instance_type": "c5.large"
            },
            "cost_optimization": {
                "use_spot_instances": True,
                "spot_price_limit": 0.1,
                "on_demand_baseline": 1
            }
        }
        
        response = test_client.post("/api/v1/pipeline/autoscaling/configure", json=autoscaling_config)
        assert response.status_code == 200


if __name__ == "__main__":
    """ì¸ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (Rust ìŠ¤íƒ€ì¼)"""
    import subprocess
    import sys
    
    result = subprocess.run([
        sys.executable, "-m", "pytest", __file__, "-v", "--tb=short", "-x"
    ])
    
    if result.returncode == 0:
        print("âœ… CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ëª¨ë“  ì¼€ì´ìŠ¤ í†µê³¼")
    else:
        print("âŒ CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1)