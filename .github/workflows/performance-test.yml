name: Performance Test

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 3 * * 1'  # Îß§Ï£º ÏõîÏöîÏùº ÏÉàÎ≤Ω 3Ïãú

env:
  REGISTRY: registry.jclee.me
  IMAGE_NAME: health-management-system

jobs:
  load-test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: health_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install locust pytest-benchmark
      
      - name: Start application
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/health_test
          REDIS_URL: redis://localhost:6379/0
          JWT_SECRET: test-secret
        run: |
          uvicorn src.app:create_app --factory --host 0.0.0.0 --port 8000 &
          sleep 10
          curl http://localhost:8000/health
      
      - name: Run Locust load test
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          import random
          
          class HealthSystemUser(HttpUser):
              wait_time = between(1, 3)
              
              def on_start(self):
                  # Î°úÍ∑∏Ïù∏
                  response = self.client.post("/api/v1/auth/login", json={
                      "username": "admin",
                      "password": "admin123"
                  })
                  if response.status_code == 200:
                      self.token = response.json().get("access_token")
                      self.client.headers.update({"Authorization": f"Bearer {self.token}"})
              
              @task(3)
              def health_check(self):
                  self.client.get("/health")
              
              @task(5)
              def get_workers(self):
                  self.client.get("/api/v1/workers/")
              
              @task(2)
              def get_worker_detail(self):
                  worker_id = random.randint(1, 100)
                  self.client.get(f"/api/v1/workers/{worker_id}")
              
              @task(1)
              def create_worker(self):
                  self.client.post("/api/v1/workers/", json={
                      "name": f"Test Worker {random.randint(1000, 9999)}",
                      "employee_id": f"EMP{random.randint(10000, 99999)}",
                      "department": "Í±¥ÏÑ§Î∂Ä",
                      "position": "ÏûëÏóÖÏûê"
                  })
          EOF
          
          # Î∂ÄÌïò ÌÖåÏä§Ìä∏ Ïã§Ìñâ (100Î™Ö ÏÇ¨Ïö©Ïûê, 5Î∂ÑÍ∞Ñ)
          locust --headless --users 100 --spawn-rate 10 --run-time 5m \
                 --host http://localhost:8000 \
                 --html performance-report.html \
                 --csv performance-metrics
      
      - name: Analyze performance metrics
        run: |
          echo "## üìä Performance Test Results" > performance-summary.md
          echo "" >> performance-summary.md
          
          # CSV ÌååÏùº Î∂ÑÏÑù
          if [ -f performance-metrics_stats.csv ]; then
            echo "### Request Statistics" >> performance-summary.md
            echo '```' >> performance-summary.md
            cat performance-metrics_stats.csv | column -t -s, >> performance-summary.md
            echo '```' >> performance-summary.md
          fi
          
          # ÏÑ±Îä• Í∏∞Ï§Ä Ï≤¥ÌÅ¨
          python << 'EOF'
          import csv
          import sys
          
          with open('performance-metrics_stats.csv', 'r') as f:
              reader = csv.DictReader(f)
              failed = False
              for row in reader:
                  if row['Type'] == 'Aggregated':
                      avg_response = float(row['Average Response Time'])
                      failures = float(row['Failure Count'])
                      
                      print(f"Average Response Time: {avg_response}ms")
                      print(f"Failure Count: {failures}")
                      
                      if avg_response > 500:  # 500ms Ïù¥ÏÉÅÏù¥Î©¥ Ïã§Ìå®
                          print("‚ùå Performance degradation detected!")
                          failed = True
                      if failures > 100:  # Ïã§Ìå® 100Í∞ú Ïù¥ÏÉÅÏù¥Î©¥ Ïã§Ìå®
                          print("‚ùå Too many failures!")
                          failed = True
              
              sys.exit(1 if failed else 0)
          EOF
      
      - name: Upload performance reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-reports
          path: |
            performance-report.html
            performance-metrics*.csv
            performance-summary.md

  # API Response Time Test
  api-benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      
      - name: Install autocannon
        run: npm install -g autocannon
      
      - name: Pull and run container
        run: |
          docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest || true
          docker-compose -f docker-compose.no-watchtower.yml up -d
          sleep 30
      
      - name: Run API benchmark
        run: |
          # Health endpoint benchmark
          autocannon -c 100 -d 30 -p 10 \
            --renderStatusCodes \
            --json \
            http://localhost:3001/health > health-benchmark.json
          
          # API endpoint benchmark
          autocannon -c 50 -d 30 -p 5 \
            --renderStatusCodes \
            --json \
            http://localhost:3001/api/v1/workers/ > api-benchmark.json
      
      - name: Analyze benchmark results
        run: |
          node << 'EOF'
          const fs = require('fs');
          
          function analyzeBenchmark(file, name) {
            const data = JSON.parse(fs.readFileSync(file, 'utf8'));
            console.log(`\n### ${name} Benchmark Results`);
            console.log(`- Requests/sec: ${data.requests.average}`);
            console.log(`- Latency (avg): ${data.latency.average}ms`);
            console.log(`- Latency (p99): ${data.latency.p99}ms`);
            console.log(`- Errors: ${data.errors}`);
            
            // ÏÑ±Îä• Í∏∞Ï§Ä
            if (data.latency.p99 > 1000) {
              console.error(`‚ùå P99 latency too high: ${data.latency.p99}ms`);
              process.exit(1);
            }
            if (data.errors > 0) {
              console.error(`‚ùå Errors detected: ${data.errors}`);
              process.exit(1);
            }
          }
          
          analyzeBenchmark('health-benchmark.json', 'Health Check');
          analyzeBenchmark('api-benchmark.json', 'API Endpoint');
          EOF
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            *-benchmark.json

  # Memory Leak Detection
  memory-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install memory_profiler pytest-memray
      
      - name: Run memory tests
        run: |
          cat > test_memory.py << 'EOF'
          import asyncio
          import pytest
          from memory_profiler import profile
          from src.app import create_app
          from src.models import Worker
          
          @profile
          def test_memory_leak_in_worker_creation():
              """Î©îÎ™®Î¶¨ ÎàÑÏàò ÌÖåÏä§Ìä∏"""
              app = create_app()
              
              # 1000Í∞ú ÏõåÏª§ ÏÉùÏÑ±/ÏÇ≠Ï†ú Î∞òÎ≥µ
              for i in range(1000):
                  worker = Worker(
                      name=f"Test Worker {i}",
                      employee_id=f"EMP{i:05d}"
                  )
                  del worker
              
              return True
          
          if __name__ == "__main__":
              test_memory_leak_in_worker_creation()
          EOF
          
          python test_memory.py > memory-report.txt
      
      - name: Check memory usage
        run: |
          if grep -q "MiB" memory-report.txt; then
            echo "Memory profiling completed"
            cat memory-report.txt
          fi