"""
Redis Cache Service Integration Tests
Inline tests for caching functionality
"""

import os
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, Any

from src.testing import (
    integration_test, run_inline_tests,
    create_test_environment, measure_performance
)
from src.services.cache import CacheTTL, cache_key_builder


class IntegrationTestRedisCache:
    """Redis ìºì‹œ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @integration_test
    @measure_performance
    async def test_cache_basic_operations(self):
        """ìºì‹œ ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        async with create_test_environment() as env:
            cache = env["cache"]
            
            # 1. SET/GET í…ŒìŠ¤íŠ¸
            test_key = "test:basic:string"
            test_value = "Hello, ì•ˆë…•í•˜ì„¸ìš”!"
            
            await cache.set(test_key, test_value, ttl=60)
            retrieved = await cache.get(test_key)
            assert retrieved == test_value
            
            # 2. ë³µì¡í•œ ê°ì²´ ìºì‹±
            complex_data = {
                "id": 123,
                "name": "í™ê¸¸ë™",
                "data": {
                    "department": "ê°œë°œë¶€",
                    "skills": ["Python", "FastAPI", "Redis"],
                    "joined_at": datetime.now().isoformat()
                }
            }
            
            complex_key = "test:complex:object"
            await cache.set(complex_key, json.dumps(complex_data), ttl=60)
            
            retrieved_json = await cache.get(complex_key)
            retrieved_data = json.loads(retrieved_json)
            assert retrieved_data["name"] == "í™ê¸¸ë™"
            assert len(retrieved_data["data"]["skills"]) == 3
            
            # 3. DELETE í…ŒìŠ¤íŠ¸
            await cache.delete(test_key)
            assert await cache.get(test_key) is None
            
            # 4. EXISTS í…ŒìŠ¤íŠ¸
            assert await cache.exists(complex_key) is True
            assert await cache.exists("non:existent:key") is False
    
    @integration_test
    async def test_cache_ttl_expiration(self):
        """ìºì‹œ TTL ë§Œë£Œ í…ŒìŠ¤íŠ¸"""
        async with create_test_environment() as env:
            cache = env["cache"]
            
            # ì§§ì€ TTL ì„¤ì •
            short_ttl_key = "test:ttl:short"
            await cache.set(short_ttl_key, "expire soon", ttl=1)
            
            # ì¦‰ì‹œ í™•ì¸
            assert await cache.get(short_ttl_key) == "expire soon"
            
            # 2ì´ˆ ëŒ€ê¸°
            await asyncio.sleep(2)
            
            # ë§Œë£Œ í™•ì¸
            assert await cache.get(short_ttl_key) is None
            
            # CacheTTL enum ì‚¬ìš©
            ttl_test_cases = [
                (CacheTTL.WORKER_LIST, "worker:list:data"),
                (CacheTTL.WORKER_DETAIL, "worker:123:detail"),
                (CacheTTL.HEALTH_EXAM, "health:exam:456"),
                (CacheTTL.STATISTICS, "stats:dashboard:2024")
            ]
            
            for ttl, key in ttl_test_cases:
                await cache.set(key, f"data_for_{key}", ttl=ttl.value)
                assert await cache.exists(key) is True
    
    @integration_test
    async def test_cache_invalidation_patterns(self):
        """ìºì‹œ ë¬´íš¨í™” íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
        async with create_test_environment() as env:
            cache = env["cache"]
            
            # 1. íŒ¨í„´ ê¸°ë°˜ ì‚­ì œ
            # ê·¼ë¡œì ê´€ë ¨ ìºì‹œ ìƒì„±
            worker_keys = [
                "worker:list:page:1",
                "worker:list:page:2",
                "worker:detail:100",
                "worker:detail:101",
                "worker:stats:department:dev"
            ]
            
            for key in worker_keys:
                await cache.set(key, f"data_{key}", ttl=300)
            
            # ëª¨ë“  worker:list:* ì‚­ì œ
            # Redisì—ì„œ ì§ì ‘ íŒ¨í„´ ì‚­ì œ (ì‹¤ì œ êµ¬í˜„ì‹œ ì£¼ì˜ í•„ìš”)
            list_keys = [k for k in worker_keys if k.startswith("worker:list:")]
            for key in list_keys:
                await cache.delete(key)
            
            # ê²€ì¦
            for key in worker_keys:
                exists = await cache.exists(key)
                if key.startswith("worker:list:"):
                    assert not exists, f"{key} should be deleted"
                else:
                    assert exists, f"{key} should still exist"
            
            # 2. ì—°ê´€ ìºì‹œ ë¬´íš¨í™”
            # ê·¼ë¡œì ì—…ë°ì´íŠ¸ì‹œ ê´€ë ¨ ìºì‹œ ëª¨ë‘ ì‚­ì œ
            worker_id = 100
            related_keys = [
                f"worker:detail:{worker_id}",
                f"worker:health:history:{worker_id}",
                f"worker:consultations:{worker_id}",
                "worker:list:page:1",  # ëª©ë¡ë„ ë¬´íš¨í™”
                "stats:dashboard:total"  # í†µê³„ë„ ë¬´íš¨í™”
            ]
            
            # ê´€ë ¨ ìºì‹œ ìƒì„±
            for key in related_keys:
                await cache.set(key, "cached_data", ttl=300)
            
            # ë¬´íš¨í™”
            for key in related_keys:
                await cache.delete(key)
            
            # ëª¨ë‘ ì‚­ì œ í™•ì¸
            for key in related_keys:
                assert await cache.exists(key) is False
    
    @integration_test
    @measure_performance
    async def test_cache_stampede_prevention(self):
        """ìºì‹œ ìŠ¤íƒ¬í”¼ë“œ ë°©ì§€ í…ŒìŠ¤íŠ¸"""
        async with create_test_environment() as env:
            cache = env["cache"]
            
            stampede_key = "test:stampede:heavy_query"
            lock_key = f"{stampede_key}:lock"
            
            # ë¬´ê±°ìš´ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
            async def heavy_computation():
                await asyncio.sleep(0.5)  # 500ms ì‘ì—…
                return {"result": "expensive_data", "timestamp": datetime.now().isoformat()}
            
            # ë™ì‹œ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
            async def get_or_compute(request_id: int):
                # ìºì‹œ í™•ì¸
                cached = await cache.get(stampede_key)
                if cached:
                    return json.loads(cached)
                
                # ë½ íšë“ ì‹œë„
                lock_acquired = False
                try:
                    # ê°„ë‹¨í•œ ë½ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” Redis SETNX ì‚¬ìš©)
                    lock_value = f"request_{request_id}"
                    existing_lock = await cache.get(lock_key)
                    
                    if not existing_lock:
                        await cache.set(lock_key, lock_value, ttl=5)
                        lock_acquired = True
                        
                        # ë¬´ê±°ìš´ ì‘ì—… ì‹¤í–‰
                        result = await heavy_computation()
                        
                        # ê²°ê³¼ ìºì‹±
                        await cache.set(stampede_key, json.dumps(result), ttl=60)
                        
                        return result
                    else:
                        # ë½ì„ ëª» ì–»ìœ¼ë©´ ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                        await asyncio.sleep(0.1)
                        cached = await cache.get(stampede_key)
                        if cached:
                            return json.loads(cached)
                        else:
                            # í´ë°±
                            return {"result": "fallback", "request_id": request_id}
                finally:
                    if lock_acquired:
                        await cache.delete(lock_key)
            
            # 10ê°œ ë™ì‹œ ìš”ì²­
            start_time = datetime.now()
            results = await asyncio.gather(*[
                get_or_compute(i) for i in range(10)
            ])
            duration = (datetime.now() - start_time).total_seconds()
            
            # ê²€ì¦: í•œ ë²ˆë§Œ ê³„ì‚°ë˜ì–´ì•¼ í•¨
            unique_timestamps = set(r.get("timestamp", "") for r in results if "timestamp" in r)
            assert len(unique_timestamps) <= 2, "ë„ˆë¬´ ë§ì€ ê³„ì‚°ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
            
            # ì„±ëŠ¥: 1ì´ˆ ì´ë‚´ ì™„ë£Œ
            assert duration < 1.0, f"ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ê°€ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤: {duration}ì´ˆ"
    
    @integration_test
    async def test_cache_key_generation(self):
        """ìºì‹œ í‚¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
        async with create_test_environment() as env:
            cache = env["cache"]
            
            # ë‹¤ì–‘í•œ í‚¤ ìƒì„± íŒ¨í„´
            test_cases = [
                # (prefix, params, expected_pattern)
                ("worker:list", {"page": 1, "limit": 20}, "worker:list:page:1:limit:20"),
                ("worker:detail", {"id": 123}, "worker:detail:id:123"),
                ("stats", {"year": 2024, "month": 1}, "stats:year:2024:month:1"),
                ("search", {"q": "í™ê¸¸ë™", "dept": "ê°œë°œë¶€"}, "search:q:í™ê¸¸ë™:dept:ê°œë°œë¶€")
            ]
            
            for prefix, params, expected in test_cases:
                # í‚¤ ìƒì„± (ê°„ë‹¨í•œ êµ¬í˜„)
                key_parts = [prefix]
                for k, v in sorted(params.items()):
                    key_parts.extend([k, str(v)])
                generated_key = ":".join(key_parts)
                
                # ìºì‹±
                await cache.set(generated_key, json.dumps(params), ttl=60)
                
                # ê²€ì¦
                cached = await cache.get(generated_key)
                assert cached is not None
                assert json.loads(cached) == params
    
    @integration_test
    async def test_distributed_cache_consistency(self):
        """ë¶„ì‚° ìºì‹œ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        async with create_test_environment() as env:
            cache = env["cache"]
            
            # ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤/ì„œë²„ì—ì„œ ë™ì¼ ë°ì´í„° ì ‘ê·¼ ì‹œë®¬ë ˆì´ì…˜
            shared_key = "shared:counter"
            
            # ì´ˆê¸°ê°’ ì„¤ì •
            await cache.set(shared_key, "0", ttl=300)
            
            # ë™ì‹œ ì¦ê°€ ì‘ì—…
            async def increment_counter(process_id: int):
                for _ in range(10):
                    # Get current value
                    current = await cache.get(shared_key)
                    if current:
                        value = int(current)
                        # Simulate processing
                        await asyncio.sleep(0.01)
                        # Increment and save
                        await cache.set(shared_key, str(value + 1), ttl=300)
            
            # 5ê°œ "í”„ë¡œì„¸ìŠ¤"ì—ì„œ ë™ì‹œ ì‹¤í–‰
            await asyncio.gather(*[
                increment_counter(i) for i in range(5)
            ])
            
            # ìµœì¢…ê°’ í™•ì¸ (ê²½ìŸ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ 50ì´ ì•„ë‹ ìˆ˜ ìˆìŒ)
            final_value = await cache.get(shared_key)
            final_int = int(final_value) if final_value else 0
            
            # ì¼ë¶€ ì—…ë°ì´íŠ¸ëŠ” ì†ì‹¤ë  ìˆ˜ ìˆìŒ (Redis INCR ì‚¬ìš©í•´ì•¼ í•¨)
            assert 10 <= final_int <= 50, f"ì˜ˆìƒ ë²”ìœ„ ë°–ì˜ ê°’: {final_int}"
    
    @integration_test
    async def test_cache_memory_management(self):
        """ìºì‹œ ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
        async with create_test_environment() as env:
            cache = env["cache"]
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìºì‹±
            large_data = {
                "data": "x" * 1000,  # 1KB
                "items": [{"id": i, "name": f"item_{i}"} for i in range(100)]
            }
            
            # 100ê°œ í‚¤ì— ì €ì¥
            stored_keys = []
            for i in range(100):
                key = f"large:data:{i}"
                await cache.set(key, json.dumps(large_data), ttl=300)
                stored_keys.append(key)
            
            # ëª¨ë“  í‚¤ ì¡´ì¬ í™•ì¸
            existing_count = 0
            for key in stored_keys:
                if await cache.exists(key):
                    existing_count += 1
            
            # ëŒ€ë¶€ë¶„ ì €ì¥ë˜ì–´ì•¼ í•¨
            assert existing_count >= 95, f"ì €ì¥ëœ í‚¤ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {existing_count}/100"
            
            # ì •ë¦¬
            for key in stored_keys:
                await cache.delete(key)
    
    @integration_test
    async def test_cache_performance_metrics(self):
        """ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
        async with create_test_environment() as env:
            cache = env["cache"]
            
            metrics = {
                "hits": 0,
                "misses": 0,
                "set_times": [],
                "get_times": []
            }
            
            # ì„±ëŠ¥ ì¸¡ì •
            test_data = {"key": "value", "data": list(range(100))}
            
            # SET ì„±ëŠ¥
            for i in range(100):
                key = f"perf:test:{i}"
                start = datetime.now()
                await cache.set(key, json.dumps(test_data), ttl=60)
                duration = (datetime.now() - start).total_seconds() * 1000  # ms
                metrics["set_times"].append(duration)
            
            # GET ì„±ëŠ¥ (ìºì‹œ íˆíŠ¸)
            for i in range(100):
                key = f"perf:test:{i}"
                start = datetime.now()
                result = await cache.get(key)
                duration = (datetime.now() - start).total_seconds() * 1000  # ms
                metrics["get_times"].append(duration)
                if result:
                    metrics["hits"] += 1
                else:
                    metrics["misses"] += 1
            
            # GET ì„±ëŠ¥ (ìºì‹œ ë¯¸ìŠ¤)
            for i in range(100, 200):
                key = f"perf:test:{i}"
                start = datetime.now()
                result = await cache.get(key)
                duration = (datetime.now() - start).total_seconds() * 1000  # ms
                metrics["get_times"].append(duration)
                if result:
                    metrics["hits"] += 1
                else:
                    metrics["misses"] += 1
            
            # ë©”íŠ¸ë¦­ ë¶„ì„
            avg_set_time = sum(metrics["set_times"]) / len(metrics["set_times"])
            avg_get_time = sum(metrics["get_times"]) / len(metrics["get_times"])
            hit_rate = metrics["hits"] / (metrics["hits"] + metrics["misses"])
            
            # ì„±ëŠ¥ ê¸°ì¤€
            assert avg_set_time < 10, f"SET í‰ê·  ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {avg_set_time:.2f}ms"
            assert avg_get_time < 5, f"GET í‰ê·  ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {avg_get_time:.2f}ms"
            assert hit_rate >= 0.45, f"ìºì‹œ ì ì¤‘ë¥ ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {hit_rate:.2%}"
            
            print(f"\nğŸ“Š ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
            print(f"  - í‰ê·  SET ì‹œê°„: {avg_set_time:.2f}ms")
            print(f"  - í‰ê·  GET ì‹œê°„: {avg_get_time:.2f}ms")
            print(f"  - ìºì‹œ ì ì¤‘ë¥ : {hit_rate:.2%}")


# Run inline tests if module is executed directly
if __name__ == "__main__" or os.environ.get("RUN_INTEGRATION_TESTS"):
    if __name__ == "__main__":
        asyncio.run(run_inline_tests(__name__))